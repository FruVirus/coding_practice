"""
Modeling
========

The first step of entity linking is to build a representation of the terms that you can
use in the ML models. It’s also critical to use contextual information (i.e., other
terms in the sentence) as you embed the terms.

Contextualized text representation
----------------------------------

The NER and NED models require context to correctly recognize entity type and
disambiguate, respectively. Therefore, the representation of terms must take contextual
information into account.

One way to represent text is in the form of embeddings. The embedding model needs to
consider the whole sentence/context while generating an embedding for a word to ensure
that its true meaning is captured. The embedding model needs to be bi-directional, i.e.,
it should look at the context in both the backward direction and the forward direction.

Two popular model architectures that generate term contextual embeddings are:

    1. ELMo

    2. BERT

It makes sense to use the cased version knowing that case is important in NER (generally
the upper case is used when writing names).

NER modelling
-------------

Contextual embedding as features

One quick way to utilize these contextual embeddings generated by BERT is to use them as
features in your NER modelling. Once BERT gives us token level embeddings, you can train
a classifier on top of token embeddings to predict NER classes.

Fine-tuning embeddings

Another option is to take the pre-trained models generated on a large corpus (e.g., BERT
base, BERT large, and DistilBERT) and fine-tune them based on your NER dataset to
improve the classifier quality. This makes more sense, especially when we have a large
NER labelled data set. For BERT fine-tuning, we can either fine-tune the whole model or
only top k layers, depending on how much training data you have and also on training
time (performance).

If the interviewer asks you to not use these large pre-trained models due to time or
resource constraints, you can build your own customized model based on similar concepts.

Disambiguation modeling
-----------------------

The disambiguation process consists of two phases:

    1. Candidate generation. In this phase, for each recognized entity, you will select
a subset of the knowledge-base entities as candidates that might correspond to it.

    2. Linking. In this phase, for each recognized entity, you will select a
corresponding entity from among the candidate entities. Thanks to candidate generation,
you will only have to choose from a smaller subset instead of the whole knowledge base.

Candidate generation
--------------------

The process of candidate generation requires building an index where terms are mapped to
knowledge base entities. This index will help us in the quick retrieval of candidates
for recognized entities. To build this index, you need ways to figure out what terms
should be used to index each entity in the knowledge base.

You need to build the index such that candidate generation focuses on higher recall. The
index should include all terms that could possibly refer to an entity, even if it is
something as trivial as a nickname or a less frequently used term for an entity.

Now let’s look at some of the ways to look for terms to index each entity on.

    1. You will index a knowledge base entity on all the terms in its name and their
concatenations. For instance, for the entity “Michael Irwin Jordan”, the index terms can
include “Michael Irwin Jordan”, “Michael Jordan”, “Michael”, “Irwin”, “Jordan”,
“Jordan Michael”, and so on. Now, if you encounter any of these terms in the text, you
can say that they might be referring to the entity “Michael Irwin Jordan” in the
knowledge base.

    2. You can also make use of referrals and anchor text in the knowledge-base data for
this purpose. For instance, assume that an anchor text reads “Michael I. Jordan” and
refers to the “Michael Irwin Jordan” entity in the knowledge-base. Here, you will index
the entity on the terms in the anchor text as well. This way, you will get a flavour of
a lot of different ways in which a knowledge base entity may be referred to in text such
as nicknames and abbreviations.

    3. If you are provided with a source that can give us commonly used spellings,
aliases, and synonyms for an entity’s name, then you can also use these terms for
indexing. However, if that is not the case, then you can use the embedding method.

    Embedding method. You know about some terms that link to a particular entity by
methods such as the two described above. In order to discover more terms to index a
particular entity on, you can look for words that are similar to the ones an entity is
already indexed on. The first step to finding similar words is representing all the
words in the knowledge base with the help of embeddings. The model will try to bring the
abbreviation/aliases/synonyms, that refer to the same entity closer in the embedding
space. Once you have the embedding of all the words, you can find k nearest neighboring
terms for a particular term that is already linked to an entity. These k nearest
neighboring terms can also be used to index the entity.

Linking
-------

Once you are done generating candidates from the knowledge base entities for the
recognized entity mentions in the given input sentence, you need to perform linking.

In the linking stage, you will build a model that will give us the probability of a
candidate being the true match for a recognized entity. You will select the candidate
with the highest probability and link it to the recognized entity mention.

Let’s look at the linking model’s inputs. It will receive:

    - the recognized entity mention that you want to link

    - the type of the entity mention, e.g., location, person etc.

    - the whole input sentence in which the entity mention is recognised

    - the candidate entity from the knowledge base

    - the prior: for a certain entity mention, how many times the candidate entity under
consideration is actually being referred to. For example, the anchor text “Harrison
Ford”, referred to the entity American actor 98% of the time instead of the silent film
actor “Harrison Edward Ford”. It’s important to tell the model that priors favor a
certain entity more.

    Utilization of the candidate entity in the anchor text is the “prior” in the above
example.

All of these inputs will be fed to a deep neural network (DNN) classifier, which will
give us the required probability.

This stage focuses on precision, i.e., you want to identify the correct corresponding
entity for a mention.

It is important to consider how the inputs (entity mention, sentence and candidate
entity) will be represented. The best representations are the ones given by
contextualized embeddings, which you are generating through models such as BERT and ELMO
for NER.

For example, assume that you have used BERT for NER, so you have the contextual
embedding for the recognized entity. BERT can also provide sentence embeddings (along
with term embeddings), so you can have the embedding for the entire input sentence. You
can also generate the embeddings of the candidate entities based on their representation
in the knowledge base. For instance, Wikidata has a small description of each entity in
the knowledge base, which can be used to generate an embedding for the candidate entity.

These embeddings, along with the prior and entity type, will provide the model with all
the information it needs to produce a good output.
"""
