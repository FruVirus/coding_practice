Problem Statement
=================

Problem statement
-----------------

The interviewer has asked you to build a system to show the most relevant ads to users.

Visualizing the problem
-----------------------

There are two well-known advertising platforms Google and Facebook, that run
advertisements paid for by businesses.

When you type a search query in a google.com search bar, ads appear in the search
results. Google’s search network allows advertisers to show their business
advertisements to users who are actively looking for products or services that the
advertiser provides. Within the Search Network, Google matches a keyword that is
relevant to a product or business advertiser, so that when a user issues a query, it
triggers an ad that shows up on the search page for a user to click on.

Unlike Google, Facebook ads are mostly shown on users’ news feed based on users’
interests.

Amazon.com also shows ads, generally in their search result page based on query context
as shown in the picture.

Interview questions
-------------------

The interviewer can ask the following questions about this problem, narrowing the scope
of the question each time.

    - How would you build an ML system to predict the probability of engagement for Ads?

    - How would you build an Ads relevance system for a search engine?

    - How would you build an Ads relevance system for a social network?

Note that the context can be different depending on the type of application in which we
are displaying the advertisement.

There are two categories of applications:

Search engine: Here, the query will be part of the context along with the searcher. The
system will display ads based on the search query issued by the searcher.

Social media platforms: Here, we do not have a query, but the user information (such as
location, demographics, and interests hierarchy) will be part of the context and used to
select ads. The system will automatically detect user interest based on the user’s
historical interactions (using machine learning algorithms) and display ads accordingly.

Most components will be the same for the above two discussed platforms with the main
difference being the context that is used to select and predict ad engagement.

Let’s set up the machine learning problem:

    “Predict the probability of engagement of an ad for a given user and context
(query, device, etc.)”

Metrics
=======

The metrics should ensure that these models help the overall improvement of the
platform, increase revenue, and provide value for the advertisers.

In our case, it’s a binary classification task; a user engages with ad or not. We use a
0 class label for no-engagement (with an ad) and 1 class label for engagement (with an
ad).

Like any other optimization problem, there are two types of metrics to measure the
effectiveness of our ad prediction system:

    1. Offline metrics

    2. Online metrics

Calibration
-----------

Calibration measures the ratio of average predicted rate and average empirical rate. In
other words, it is the ratio of the number of expected actions to the number of actually
observed actions.

Calibration = predicted rate / actual historically observed rate

Why do we need calibration? When we have a significant class imbalance, i.e., the
distribution is skewed towards positive and negative class, we calibrate our model to
estimate the likelihood of a data point belonging to a class.

Offline metrics
---------------

Log Loss

Let’s first go over the area under the receiver operator curve (AUC), which is a
commonly used metric for model comparison in binary classification tasks. However, given
that the system needs well-calibrated prediction scores, AUC, has the following
shortcomings in this ad prediction scenario.

    1. AUC does not penalize for “how far off” predicted score is from the actual label.
For example, let’s take two positive examples (i.e., with actual label 1) that have the
predicted scores of 0.51 and 0.7 at threshold 0.5. These scores will contribute equally
to our loss even though one is much closer to our predicted value.

    2. AUC is insensitive to well-calibrated probabilities.

Since, in our case, we need the model’s predicted score to be well-calibrated to use in
Auction, we need a calibration-sensitive metric. Log loss should be able to capture this
effectively as Log loss (or more precisely cross-entropy loss) is the measure of our
predictive error.

This metric captures to what degree expected probabilities diverge from class labels. As
such, it is an absolute measure of quality, which accounts for generating
well-calibrated, probabilistic output.

Online metrics
--------------

Overall revenue

This captures the overall revenue generated by the system for the cohorts of the user in
either an experiment or, more generally, to measure the overall performance of the
system. It’s important to call out that just measuring revenue is a very short term
approach, as we may not provide enough value to advertisers and they will move away from
the system. However, revenue is definitely one critical metric to track.

Revenue is basically computed as the sum of the winning bid value (as selected by
auction) when the predicted event happens, e.g., if the bid amounts to $0.5 and the user
clicks on the ad, the advertiser will be charged $0.5. The business won’t be charged if
the user doesn’t click on the ad.

Overall ads engagement rate

Engagement rate measures the overall action rate, which is selected by the advertiser.
Some of the actions might be:

    1. Click rate. This will measure the ratio of user clicks to ads.

    2. Downstream action rate. This will measure the action rate of a particular action
targeted by the advertiser e.g. add to cart rate, purchase rate, message rate etc.

Counter metrics

It’s important to track counter metrics to see if the ads are negatively impacting the
platform.

We want the users to keep showing engagement with the platform and ads should not hinder
that interest. That is why it’s important to measure the impact of ads on the overall
platform as well as direct negative feedback provided by users. There is a risk that
users can leave the platform if ads degrade the experience significantly.

So, for online ads experiments, we should track key platform metrics, e.g., for search
engines, is session success going down significantly because of ads? Are the average
queries per user impacted? Are the number of returning users on the platform impacted?
These are a few important metrics to track to see if there is a significant negative
impact on the platform.

Along with top metrics, it’s important to track direct negative feedback by the user on
the ad such as providing following feedback on the ad:

    1. Hide ad

    2. Never see this ad

    3. Report ad as inappropriate

These negative sentiments can lead to the perceived notion of the product as negative.

Architectural Components
========================

Architecture
------------

There will be two main actors involved in our ad prediction system - platform users and
advertiser.

1. Advertiser flow

Advertisers create ads containing their content as well as targeting, i.e., scenarios in
which they want to trigger their ads. A few examples are:

    - Query-based targeting: This method shows ads to the user based on the query terms.
The query terms can be a partial match, full match, expansion, etc.

    - User-based targeting: The ads will be subjective to the user based on a specific
region, demographic, gender, age, etc.

    - Interest-based targeting: This method shows interest-based ads. For example, the
advertiser might like to show sports-related ads to people interested in sports.

    - Set-based targeting: This type shows ads to a set of users selected by the
advertisers. For example, showing an ad to people who were previous buyers or have spent
more than ten minutes on the website.

2. User flow

As the platform user queries the system, it will look for all the potential ads that can
be shown to this user based on different targeting criteria used by the advertiser.

So, the flow of information will have two major steps as described below:

    - Advertisers create ads providing targeting information, and the ads are stored in
the ads index.

    - When a user queries the platform, ads can be selected from the index based on
their information (e.g., demographics, interests, etc.) and run through our ads
prediction system.

Ad selection
------------

The ad selection component will fetch the top k ads based on relevance (subject to the
user context) and bid from the ads index.

Ad prediction
-------------

The ad prediction component will predict user engagement with the ad (the probability
that an action will be taken on the ad if it is shown), given the ad, advertiser, user,
and context. Then, it will rank ads based on relevance score and bid.

Auction
-------

The auction mechanism then determines whether these top K relevant ads are shown to the
user, the order in which they are shown, and the price the advertisers pay if an action
is taken on the ad.

For every ad request, an auction takes place to determine which ads to show. The top
relevant ads selected by the ad prediction system are given as input to Auction. Auction
then looks at total value based on an ad’s bid as well as its relevance score. An ad
with the highest total value is the winner of the auction. The total value depends on
the following factors:

    - Bid: The bid an advertiser places for that ad. In other words, the amount the
advertiser is willing to pay for a given action such as click or purchase.

    - Budget: The advertiser’s budget for an ad

    - User engagement rate: An estimate of user engagement with the ad.

    - Ad quality score: An assessment of the quality of the ad by taking into account
feedback from people viewing or hiding the ad.

The estimated user engagement and ad quality rates combined results in the ad relevance
score. They can be combined based on different weights as selected by the platform,
e.g., if it’s important to keep positive feedback high, the ad quality rate will get a
higher weight.

The rank score is calculated based on predicted ad score (from the ad prediction
component) multiplied by the bid.

Ad rank score = Ad predicted score * bid

Ads with the highest ad rank score wins the auction and are shown to the user. Once an
ad wins the auction, the cost per engagement (CPE) or cost per click (CPC) will depend
on its ad rank score and ad rank score of the ad right below it in rank order, i.e.,

CPE = Ad rank of ad below / Ad rank score + 0.01

A general principle is that the ad will cost the minimal price that still allows it to
win the auction.

Pacing
------

Pacing an ad means evenly spending the ad budget over the selected time period rather
than trying to spend all of it at the start of the campaign.

Remember that whenever the user shows engagement with an ad, the advertiser gets charged
the bid amount of the next ad. If the ad rank score is high, the ad set can spend the
whole budget at the start of a time period (like the start of a new day, as advertisers
generally have daily budgets). This would result in a high cost per click (CPC) when the
ad load (the user engagement rate) is high.

Pacing overcomes this by dynamically changing the bid such that the ad set is evenly
spread out throughout the day and the advertiser gets maximum ROI on their campaign.
This also prevents the overall system from having a significantly high load at the start
of the day, and the budget is spent evenly throughout the campaign.

Training data generation
------------------------

We need to record the action taken on an ad. This component takes user action on the ads
(displayed after the auction) and generates positive and negative training examples for
the ad prediction component.

Funnel model approach
---------------------

For a large scale ads prediction system, it’s important to quickly select an ad for a
user based on either the search query and/or user interests. The scale can be large both
in terms of the number of ads in the system and the number of users on the platform.

To achieve the above objective, it would make sense to use a funnel approach, we
gradually move from a large set of ads to a more precise set for the next step in the
funnel.

As we go down the funnel, the complexity of the models becomes higher and the set of ads
that they run on becomes smaller. It’s also important to note that the initial layers
are mostly responsible for ads selection. On the other hand, ads prediction is
responsible for predicting a well-calibrated engagement and quality score for ads. This
predicted score is going to be utilized in the auction as well.

Let’s go over an example to see how these components will interact for the search
scenario.

    - A thirty-year old male user issues a query “machine learning”.

    - The ads selection component selects all the ads that match the targeting criteria
(user demographics and query) and uses a simple model to predict the ad’s relevance
score.

    - The ads selection component ranks the ads according to r, where
r = bid * relevance and sends the top ads to our ads prediction system.

    - The ads prediction component will go over the selected ads and uses a highly
optimized ML model to predict a precise calibrated score.

    - The ads auction component then runs the auction algorithm based on the bid and
predicted ads score to select the top most relevant ads that are shown to the user.

Feature Engineering
===================

The main actors that will play a key role in our feature engineering process are:

    1. Ad

    2. Advertiser

    3. User

    4. Context. Context refers to the engagement history, user interests, current
location, time and date, etc.

Features for the model
----------------------

Now it’s time to generate features based on these actors. The features would fall into
the following categories:

    1, Ad specific features

    2. Advertiser specific features

    3. User specific features

    4. Context specific features

    5. User-ad cross features

    6. User-advertiser cross features

Ad specific features
--------------------

    - ad_id

    - ad_content_raw_terms

    - historical_engagement_rate

        - ad_engagement_history_last_24_hrs

        - ad_engagement_history_last_7_days

    - ad_impression

    - ad_negative_engagement_rate

    - ad_embedding

    - ad_age

    - ad_bid

Advertiser specific features
----------------------------

    - advertiser_domain

    - historical_engagement_rate

    - region_wise_engagement

User specific features
----------------------

    - user_previous_search_terms

    - user_search_terms

    - age

    - gender

    - language

    - embedding_last_k_ads

    - engagement_content_type

    - engagement_days

    - platform_time_spent

    - region

Context specific features
-------------------------

    - current_region

    - time

    - device

        - screen_size

User-ad cross features
----------------------

    - embedding_similarity

    - region_wise_engagement

    - user_ad_category_histogram

    - user_ad_subcategory_histogram

    - user_gender_ad_histogram

    - user_age_ad_histogram

User-advertiser cross features
------------------------------

    - embedding_similarity

    - user_gender_advertiser_histogram

    - user_age_advertiser_histogram

Key consideration for historical engagement features
----------------------------------------------------

There are two ways to pass historical data that is partitioned on a specific key (such
as day, age, gender, etc.) as features to our models.

The first approach is to give the histogram data along with the key value to the model
for learning. For example, in the case of daily historical engagement rate, we will give
the model a histogram of seven-day user engagement from Sunday to Monday, which looks
like [0.4, 0.2, 0.21, 0.25, 0.27, 0.38, 0.42]. In addition, we will pass the current day
as a feature as well, e.g., if the day is Sunday the feature value of day will be 1, and
for Saturday the feature value will be 7. So, the model will see eight features in
total: seven more raw features for each day historical engagement rate and one feature
for the current day of the week.

The second approach for giving this historical information to our model is to create a
single feature that gives the current day engagement rate as a value, e.g., the feature
value on Sunday will be 0.4, and on Friday it will be 0.38 for the above data.

It would be better to create features for learning by using both the first and second
approaches. The models that are able to learn feature interactions effectively (such as
neural networks or decision trees) can learn the current day engagement rate by the
features described in the first approach. They might also be able to learn more
interesting patterns such as understanding weekend data interactions, previous day
interactions, etc. However, it’s better to provide the full histogram data for the model
to learn new interactions for us. But, for linear models providing the current day
engagement data as we did in the second approach is critical because they can’t learn
these interactions on their own.

Training Data Generation
========================

Training data generation through online user engagement
-------------------------------------------------------

When we show an ad to the user, they can engage with it or ignore it. Positive examples
result from users engaging with ads, e.g., clicking or adding an item to their cart.
Negative examples result from users ignoring the ads or providing negative feedback on
the ad. Advertisers will usually specify which actions to consider as positive vs.
negative.

Suppose the advertiser specifies “click” to be counted as a positive action on the ad.
In this scenario, a user-click on an ad is considered as a positive training example,
and a user ignoring the ad is considered as a negative example.

Suppose the ad refers to an online shopping platform and the advertiser specifies the
action “add to cart” to be counted as positive user engagement. Here, if the user clicks
to view the ad and does not add items to the cart, it is counted as a negative training
example.

Balancing positive and negative training examples
-------------------------------------------------

Users’ engagement with an ad can be fairly low based on the platform e.g. in case of a
feed system where people generally browse content and engage with minimal content, it
can be as low as 2 - 3%. In order to balance the ratio of positive and negative training
samples, we can randomly down sample the negative examples so that we have a similar
number of positive and negative examples.

Model recalibration
-------------------

Negative downsampling can accelerate training while enabling us to learn from both
positive and negative examples. However, our predicted model output will now be in the
downsampling space. For instance, consider that if our engagement rate is 5% and we
select only 10% negative samples, our average predicted engagement rate will be near
33%. Auction uses this predicted rate to determine order and pricing; therefore it’s
critical that we recalibrate our score before sending them to auction. The recalibration
can be done using:

q = p / (p + (1 - p) / w

Here,

q is the re-calibrated prediction score,
p is the prediction in downsampling space,
w is the negative downsampling rate.

Train test split
----------------

We need to be mindful of the fact that user engagement patterns may differ throughout
the week. Hence we will use a week’s engagement to capture all patterns during training
data generation.

Random splitting would result in utilizing future data for prediction given our data has
a time dimension, i.e., we can utilize engagement on historical ads to predict future ad
engagement. Hence we will train the model on data from the one-time interval and
validate it on the data from its succeeding time interval.

Ad Selection
============

The main goal of the ads selection component is to narrow down the set of ads that are
relevant for a given query. In a search-based system, the ads selection component is
responsible for retrieving the top relevant ads from the ads database (built using all
the active ads in the system) according to the user and query context. In a feed-based
system, the ads selection component will select the top k relevant ads based more on
user interests than search terms.

Based on our discussions about the funnel-based approach for modeling, it would make
sense to structure the ad selection process in the following three phases:

    - Phase 1: Quick selection of ads for the given query and user context according to
selection criteria

    - Phase 2: Rank these selected ads based on a simple and fast algorithm to trim ads.

    - Phase 3: Apply the machine learning model on the trimmed ads to select the top
ones.

Phase 1: Selection of ads
-------------------------

The first key requirement to be able to achieve the quick selection objective is to have
the ads stored in a system that is fast and enables complex selection criteria. This is
where building an in-memory index to store the ads will be massively helpful. Index
allows us to fetch ads quickly based on different targeting and relevance information
from the user. We will index ads on all possible indexing terms that can be used for
selection e.g. targeted terms, city, state, country, region, age etc.

In a search-based system, the selection query in this case will look like:

    (term = "machine learning")
            and
    (age = "\*" or age contains "25")
            and
    (gender = "\*" or gender = "male")
            and
            ...
            and
    (has_budget = True)

In a feed-based system, the selection query would look like:

    (interest = "Computer Science" or interest = "Football" or interest = "*")
            and
    (age = "\*" or age contains "25")
            and
    (gender = "\*" or gender = "male")
            and
            ...
            and
    (has_budget = True)

Phase 2: Narrow down selection set to top relevant ads
------------------------------------------------------

The eventual ranking of ads is going to be based on (bid * predicted score). We already
know the bid at this point and can use the prior engagement score (CPE) based on the ad,
advertiser, ad type, etc. as our predicted score:

Ad Score = Bid * Prior CPE Score

For a new ad or advertiser where the system doesn’t have good prior scores, a slightly
higher score can be given to ads also called score boost. We can use time decay to
reduce it. The ranking might look like the following with a new ad boost of 10% to CPE
score for the first forty-eight hours of ad with time decay.

if ad_age < 48:
    boost = 0.1 * (1 - ad_age / 48)
    ad_score =  bid * (1 + boost) * CPE

Based on these prior scores, we can narrow down ads from our large selected set to the
top k ads. In phase 3, we will use a much stronger predictor than prior scores on the
top selected ads from phase 2.

Phase 3: Ranking of selected ads using a simplistic model
---------------------------------------------------------

The target is to select the top k relevant ads from the set given by phase 2. We will
use training data and dense features to train this model to predict engagement scores
better and minimize our log loss error.

At evaluation time, we will get a new predicted engagement score for ranking of ads. Ads
will be sorted based on this prediction CPE, which is the (bid * CPE) score, as we did
in phase 2. However, our predicted CPE score should be a far better prediction than
using historical priors as we did in phase 2.

As we progress down the funnel, the complexity of the models increases, and the number
of results decrease. Within the ad selection component, we adopt the same funnel based
approach. First, we select ads in phase 1, then rank them based on prior scores in phase
2. Finally, we rank them using an efficient model in phase 3 to come up with the top ads
that we want to send to the ad prediction stage.

How does the system scale?
--------------------------

Note that our index is sharded, i.e., it runs on multiple machines and every machine
selects the top k ads based on the prior score. Each machine then runs a simplistic
logistic regression model built on dense features (there are no sparse features to keep
the model size small) to rank ads.

The number of partitions (shards) depends on the size of the index. A large index
results in more partitions as compared to a smaller index. Also, the system load
measured in queries per second (QPS) decides how many times the partition is replicated.

Consider a scenario where the size of the index is 1 tera-byte and memory of a single
shard is 250 giga-bytes. The index data gets distributed in four partitions. Each
partition selects ads that satisfy the selection criteria. Then, we use the prior score
of the selected ads and select top five-hundred ads based on that score. To further
narrow down the ad set, we run logistic regression which returns the top 50 ads. The top
fifty ranked ads from each of the four partitions (200 ads in total) are fed to the ad
prediction component.

Note that for each level we are selecting top “k” ads. The selection of the number “k”
for each level is very arbitrary. It is based on experimentation and system available
load/capacity.

The ad set returned from the ad selection component is further ranked by the ad
prediction component.

Ad Prediction
=============

The ad prediction component has to make predictions for the final set of candidate
selected ads. It needs to be robust and adaptive and should be able to learn from
massive data volume.

Modeling approach
-----------------

Ads are generally short-lived. So, our predictive model is going to be deployed in a
dynamic environment where the ad set is continuously changing over time.

Given this change in an ad set, keeping the model up to date on the latest ads is
important. In other words, model performance will degrade with each passing day if it
isn’t refreshed frequently.

Online learning
---------------

If we have to plot the log loss of the model, we can observe the degradation of
prediction accuracy (increase of log loss) because of the increased delay between the
model training and test set.

One approach to minimize this loss could be refreshing the model more frequently e.g.
training it every day on new data accumulated since the previous day.

However, a more practical and efficient approach would be to keep refreshing the model
with the latest impressions and engagements after regular internals (e.g. 15 mins, 30
mins, etc.). This is generally referred to as online learning or active learning.

In this approach, we train a base model and keep adding new examples on top of it with
every ad impression and engagement. From an infrastructure perspective, we now need a
mechanism that collects the recent ad data and merges it with the current model.

The following figure shows some key components that are critical for enabling online
learning. We need a mechanism that generates the latest training examples via an online
joiner. Our training data generator will take these examples and generate the right
feature set for them. The model trainer will then receive these new examples to refresh
the model using stochastic gradient descent. This forms a tightly closed loop where
changes in the feature distribution or model output can be detected, learned on, and
improved in short successions. Note that the refresh of the model doesn’t have to be
instantaneous, and we can do it in same batches at a certain frequency, e.g., every 30
mins, 60 mins etc.

Model for online learning
-------------------------

One model that easily supports online learning and has the ability to update it using
stochastic gradient descent using mini-batches is logistic regression.

Auto non-linear feature generation
----------------------------------

One potential drawback is that simple logistic regression (generalized linear model)
relies on manual effort to create complex feature crosses and generating non-linear
features. Manually creating such features is cumbersome and will mostly be restricted to
modeling the relationship between two or three features. On the other hand, tree and
neural network-based models are really good at generating complex relationships among
features when optimizing the model.

To overcome this, we can use additive trees and neural networks to find such complex
feature crosses and non-linear feature relationships in data, and then these features
are input to our logistic regression model.

Additive trees

To capture complex feature crosses and non-linear relationships from the given raw
features, additive trees can be extremely handy. We can train a model to predict the
probability of engagement using trees and minimize our loss function.

We can represent an additive tree to generate features by taking into account triggered
leaf nodes, e.g., let’s say that we have one-hundred trees in our ensemble with four
leaf nodes each. This will result in 100 * 4 = 400 leaf nodes in which one-hundred nodes
will trigger for every example. This binary vector itself captures the prediction (or
learning) of our tree ensemble. We can just feed this vector as one input feature to our
logistic regression model.

Neural Network

A deep neural network can be trained on the raw input features to predict our ads
engagement to generate features for our logistic regression model.

The following examples show a network trained with two hidden layers to predict our
engagement rate. Once trained, we can use the activation from the last hidden layer to
feed in as input feature vector to our logistic regression model.

So, let’s combine the above two ideas together:

    - We train additive trees and neural network to predict non-linear complex
relationships among our features. We then use these models to generate features.

    - We use raw features and features generated in the previous step to train a logic
regression model.

The above two steps together solve both of our problems to capture complex non-linear
relationships and also enable us to use online learning to refresh the model frequently
for ads that are fast-changing and dynamic in nature.