15 Dynamic Programming
======================

Dynamic programming typically applies to optimization problems in which we make a set of
choices in order to arrive at an optimal solution. As we make each choice, sub-problems
of the same form often arise. Dynamic programming is effective when a given sub-problem
may arise from more than one partial set of choices; the key technique is to store the
solution to each sub-problem in case it should reappear. This simple idea can sometimes
transform exponential-time algorithms into polynomial-time algorithms.

Dynamic programming, like the divide-and-conquer method, solves problems by combining
the solutions to subproblems. Divide-and-conquer algorithms partition the problem into
disjoint subproblems, solve the subproblems recursively, and then combine their
solutions to solve the original problem. In contrast, dynamic programming applies when
the subproblems overlap---that is, when subproblems share subsubproblems. In this
context, a divide-and-conquer algorithm does more work than is necessary, repeatedly
solving the common subsubproblems. A dynamic-programming algorithm solves each
subsubproblem just once and then saves its answer in a table, thereby avoiding the
work of recomputing the answer every time it solves each subsubproblem.

We typically apply dynamic programming to optimization problems. Such problems can have
many possible solutions. Each solution has a value, and we wish to find a solution with
the optimal (minimum or maximum) value. We call such a solution an optimal solution to
the problem, as opposed to the optimal solution, since there may be several solutions
that achieve the optimal value.

When developing a dynamic-programming algorithm, we follow a sequence of four steps:

1. Characterize the structure of an optimal solution.
2. Recursively define the value of an optimal solution.
3. Compute the value of an optimal solution, typically in a bottom-up fashion.
4. Construct an optimal solution from computed information.

Steps 1 - 3 form the basis of a dynamic-programming solution to a problem. If we need
only the value of an optimal solution, and not the solution itself, then we can omit
step 4. When we do perform step 4, we sometimes maintain additional information during
step 3 so that we can easily construct an optimal solution.

15.3 Elements of dynamic programming
====================================

The two key ingredients that an optimization problem must have in order for dynamic
programming to apply are: 1) optimal substructure and 2) overlapping subproblems.

Optimal substructure
--------------------

The first step in solving an optimization problem by dynamic programming is to
characterize the structure of an optimal solution. A problem exhibits optimal
substructure if an optimal solution to the problems contains within it optimal solutions
to subproblems. In dynamic programming, we build an optimal solution to the problem from
optimal solutions to subproblems. Consequently, we must take care to ensure that the
range of subproblems we consider includes those used in an optimal solution.

Optimal substructure varies across problem domains in two ways:

1. how many subproblems an optimal solution to the original problem uses, and
2. how many choices we have in determining which subproblem(s) to use in an optimal
solution.

Informally, the running time of a dynamic-programming algorithm depends on the product
of two factors: 1. the number of subproblems overall and how many choices we look at for
each subproblem. In rod cutting, we have Theta(n) subproblems overall, and at most n
choices to examine for each, yield an O(n^2) running time. Matrix-chain multiplication
has Theta(n^2) subproblems overall, and in each we had at most n - 1 choices, giving an
O(n^3) running time.

Dynamic programming often uses optimal substructure in a bottom-up fashion. That is, we
first find optimal solutions to subproblems and, having solved the subproblems, we find
an optimal solution to the problem. Finding an optimal solution to the problem entails
making a choice among subproblems as to which we will use in solving the problem. The
cost of the problem solution is usually the subproblem costs plus a cost that is
directly attributable to the choice itself.

One major difference between greedy algorithms and dynamic programming is that instead
of first finding optimal solutions to subproblems and then making an informed choice,
greedy algorithms first make a "greedy" choice---the choice that looks best at the
time---and then solve a resulting subproblem, without bothering to solve all possible
related smaller subproblems.

Subtleties
----------

In order for dynamic programming to apply, the solutions to subproblems must be
independent of one another. That is, the solution to one subproblem does not affect the
solution to another subproblem of the same problem. Looked at another way, using
resources in solve one subproblem renders them unavailable for the other subproblem.

Overlapping subproblems
-----------------------

The second ingredient that an optimization problem must have for dynamic programming to
apply is that the space of subproblems must be "small" in the sense that a recursive
algorithm for the problem solves the same subproblems over and over, rather than always
generating new subproblems. Typically, the total number of distinct subproblems is a
polynomial in the input size. When a recursive algorithm revisits the same problem
repeatedly, we say that the optimization problem has overlapping subproblems. In
contrast, a problem for which a divide-and-conquer approach is suitable usually
generates brand-new problems at each stpe of the recursion. Dynamic-programming
algorithms typically take advantage of overlapping subproblems by solving each
subproblem once and then storing the solution in a table where it can be looked up when
needed, using constant time per lookup.

NB: Two subproblems of the same problem are independent if they do not share resources.
Two subproblems are overlapping if they are really the same subproblem that occurs as a
subproblem of different problems.

Whenever a recursion tree for the natural recursive solution to a problem contains the
same subproblem repeatedly, and the total number of distinct subproblems is small,
dynamic programming can improve efficiency, sometimes dramatically.

Reconstructing an optimal solution
----------------------------------

As a practical matter, we often store which choice we made in each subproblem in a table
so that we do not have to reconstruct this information from the costs that we stored.

Memoization
-----------

As in the bottom-up approach, we maintain a table with subproblem solutions, but the
control structure for filling in the table is more like the recursive algorithm.

A memoized recursive algorithm maintains an entry in a table for the solution to each
subproblem. Each table entry initially contains a special value to indicate that the
entry has yet to be filled in. When the subproblem is first encountered as the recursive
algorithm unfolds, its solution is computed and then stored in the table. Each
subsequent time that we encounter this subproblem, we simply look up the value stored in
the table and return it.

This approach presupposes that we know the set of all possible subproblem parameters and
that we have established the relationship between table positions and subproblems.
Another, more general, approach is to memoize by using hashing with the subproblem
parameters as keys.

In general practice, if all subproblems must be solved at least once, a bottom-up
dynamic-programming algorithm usually outperforms the corresponding top-down memoized
algorithm by a constant factor, because the bottom-up algorithm has no overhead for
recursion and less overhead for maintaining the table. Alternatively, if some
subproblems in the subproblem space need not be solved at all, the top-down approach
has the advantage of solving only those subproblems that are definitely required.