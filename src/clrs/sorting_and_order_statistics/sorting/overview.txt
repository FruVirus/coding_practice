II Sorting and Order Statistics
===============================

Introduction
============

The sorting problem takes as input a sequence of n numbers <a_1, a_2, ..., a_n> and
outputs a permutation (reordering) <a_1', a_2', ..., a_n'> of the input sequence such
that a_1' <= a_2' <= ... <= a_n'.

The input sequence is usually an n-element array, although it may be represented in some
other fashion, such as a linked list.

The structure of the data
-------------------------

In practice, the numbers to be sorted are rarely in isolated values. Each is usually
part of a collection of data called a record. Each record contains a key, which is the
value to be sorted. The remainder of the record consists of satellite data, which are
usually carried around with the key. In practice, when a sorting algorithm permutes the
keys, it must permute the satellite data as well. If each record includes a large amount
of satellite data, we often permute an array of pointers to the records rather than the
records themselves in order to minimize data movement.

A sorting algorithm describes the method by which we determine the sorted order,
regardless of whether we are sorting individual numbers or large records containing many
bytes of satellite data.

Sorting algorithms
------------------

A sorting algorithm sorts in place if only a constant number of elements of the input
array are ever stored outside of the array.

Insertion sort, merge sort, heap sort, and quick sort are all comparison sorts: they
determine the sorted order of an input array by comparing elements. Comparison sorting
algorithms have a lower bound of Omega(n * lg n) on their worst-case running times.

We can beat this lower bound of Omega(n * lg n) if we can gather information about the
sorted order of the input by means other than comparing elements. The counting sort
algorithm, for example, assumes that the input numbers are in the set {0, 1, ..., k}. By
using array indexing as a tool for determining relative order, counting sort can sort n
numbers in Theta(k + n) time. Thus, when k = O(n), counting sort runs in time that is
linear in the size of the input array. A related algorithm, radix sort, can be used to
extend the range of counting sort. If there are n integers to sort, each integer has d
digits, and each digit can take on up to k possible values, then radix sort can sort the
numbers in Theta (d * (n + k)) time. When d is a constant and k is O(n), radix sort runs
in linear time. A third algorithm, bucket sort, requires knowledge of the probabilistic
distribution of numbers in the input array. It can sort n real numbers uniformly
distributed in the half-open interval [0, 1) in average-case O(n) time.

8 Sorting in Linear Time
========================

Comparison sorts make comparisons (i.e., <, >, =, etc.) between the input elements in
order to sort them. Any comparison sort must make Omega(n * lg n) comparisons in the
worst case to sort n elements. Thus, merge sort and heapsort are asymptotically optimal,
and no comparison sort exists that is faster by more than a constant factor.

Counting sort, radix sort, and bucket sort run in linear time. Of course, these
algorithms use operations other than comparisons to determine the sorted order.
Consequently, the Omega(n * lg n) lower bound does not apply to them.

8.1 Lower bounds for sorting
============================

In a comparison sort, we use only comparisons between elements to gain order information
about an input sequence <a_1, a_2, ..., a_n>. We may not inspect the values of the
elements or gain order information about them in any other way.

When to Use the Different Sorting Algorithms
============================================

First, a definition, since it's pretty important: A stable sort is one that's guaranteed
not to reorder elements with identical keys.

Quick sort: When you don't need a stable sort and average case performance matters more
than worst case performance. A quick sort is O(N log N) on average, O(N^2) in the worst
case. A good implementation uses O(log N) auxiliary storage in the form of stack space
for recursion.

Merge sort: When you need a stable, O(N log N) sort, this is about your only option. The
only downsides to it are that it uses O(N) auxiliary space and has a slightly larger
constant than a quick sort. There are some in-place merge sorts, but AFAIK they are all
either not stable or worse than O(N log N). Even the O(N log N) in place sorts have so
much larger a constant than the plain old merge sort that they're more theoretical
curiosities than useful algorithms.

Heap sort: When you don't need a stable sort and you care more about worst case
performance than average case performance. It's guaranteed to be O(N log N), and uses
O(1) auxiliary space, meaning that you won't unexpectedly run out of heap or stack space
on very large inputs.

Insertion sort: When N is guaranteed to be small, including as the base case of a quick
sort or merge sort. While this is O(N^2), it has a very small constant and is a stable
sort.

Bubble sort: When you're doing something quick and dirty and for some reason you can't
just use the standard library's sorting algorithm. The only advantage these have over
insertion sort is being slightly easier to implement.

Counting sort: When you are sorting integers with a limited range.

Radix sort: When log(N) is significantly larger than K, where K is the number of radix
digits.

Bucket sort: When you can guarantee that your input is approximately uniformly
distributed.