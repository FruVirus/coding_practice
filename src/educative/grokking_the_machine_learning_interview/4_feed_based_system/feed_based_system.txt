Problem Statement
=================

Problem statement
-----------------

The interviewer has asked you to design a Twitter feed system that will show the most
relevant tweets for a user based on their social graph.

Visualizing the problem
-----------------------

User A is connected to other people/businesses on the Twitter platform. They are
interested in knowing the activity of their connections through their feed.

In the past, a rather simplistic approach has been followed for this purpose. All the
Tweets generated by their followees since user A’s last visit were displayed in reverse
chronological order.

However, this reverse-chronological order feed display often resulted in user A missing
out on some Tweets that they would have otherwise found very engaging. Twitter
experiences a large number of daily active users, and as a result, the amount of data
generated on Twitter is torrential. Therefore, a potentially engaging Tweet may have
gotten pushed further down in the feed because a lot of other Tweets were posted after
it.

Hence, to provide a more engaging user experience, it is crucial to rank the most
relevant Tweets above the other ones based on user interests and social connections.
Therefore, the feed order is now based on relevance ranking.

Scale of the problem
--------------------

Now that you know the problem at hand, let’s define the scope of the problem:

    1. Consider that there are five-hundred million daily active users.

    2. On average, every user is connected to one-hundred users.

    3. Every user fetches their feed ten times in a day.

Five-hundred million daily active users, each fetching their feed ten times daily, means
that your Tweet ranking system will run five billion times per day.

Finally, let’s set up the machine learning problem:

    “Given a list of tweets, train an ML model that predicts the probability of
engagement of tweets and orders them based on that score”

Metrics
=======

The feed-ranking system aims to maximize user engagement.

User actions
------------

The following are some of the actions that the user will perform on their tweet,
categorized as positive and negative actions.

Positive user actions

    - Time spent viewing the tweet

    - Liking a Tweet

    - Retweeting

    - Commenting on a Tweet

Negative user actions

    - Hiding a Tweet

    - Reporting Tweets as inappropriate

Now, you need to look at these different forms of engagements on the feed to see if your
feed ranking system did a good job.

User engagement metrics
-----------------------

User actions on the platform can help generate useful statistics.

Selecting feed optimization metric
----------------------------------

An important thing to understand in selecting a topline is that it’s scientific as well
as a business-driven decision.

The business might want to focus on one aspect of user engagement. For instance, Twitter
can decide that the Twitter community needs to engage more actively in a dialogue. So,
the topline metric would be to focus more on the number of comments on the Tweets. If
the average number of comments per user increases over time, it means that the feed
system is helping the business objective.

Similarly, Twitter might want to shift its focus to overall engagement. Then their
objective will be to increase average overall engagement, i.e., comments, likes, and
retweets. Alternatively, the business may require to optimize for the time spent on the
application. In this case, time spent on Twitter will be the feed system metric.

Negative engagement or counter metric
-------------------------------------

For any system, it’s super important to think about counter metrics along with the key,
topline ones. In a feed system, users may perform multiple negative actions such as
reporting a Tweet as inappropriate, block a user, hide a Tweet, etc. Keeping track of
these negative actions and having a metric such as average negative action per user is
also crucial to measure and track.

Weighted engagement
-------------------

More often than not, all engagement actions are equally important. However, some might
become more important at a particular point in time, based on changing business
objectives. For example, to have an engaged audience, the number of comments might be
more critical rather than just likes. As a result, we might want to have different
weights for each action and then track the overall engagement progress based on that
weighted sum. So, the metric would become a weighted combination of these user actions.
These weights are assigned, keeping in mind the respective importance of different forms
of engagement towards the business objectives.

The user engagements are aggregated across all users’ feeds over a specific period of
time. In the above diagram, two-thousand tweets were viewed in a day on Twitter. There
were a total of seventy likes, eighty comments, twenty retweets, and five reports. The
weighted impact of each of these user engagements is calculated by multiplying their
occurrence aggregate by their weights. In this instance, Twitter is focusing on
increasing “likes” the most. Therefore, “likes” have the highest weight. Note that the
negative user action, i.e., “report”, has a negative weight to cast its negative impact
on the score.

The weighted impacts are then summed up to determine the score. The final step is to
normalize the score with the total number of active users. This way, you obtain the
engagement per active user, making the score comparable. The reason is that the decrease
in score A may just be the effect of less active users (i.e., five-hundred active users
instead of one-hundred active users).

Architectural Components
========================

Tweet selection
---------------

This component performs the first step in generating a user’s feed, i.e., it fetches a
pool of Tweets from the user’s network (the followees), since their last login. This
pool of Tweets is then forwarded to the ranker component.

Ranker
------

The ranker component will receive the pool of selected Tweets and predict their
probability of engagement. The Tweets will then be ranked according to their predicted
engagement probabilities for display on user A’s feed. We can:

    1. Train a single model to predict the overall engagement on the tweet.

    2. Train separate models. Each model can focus on predicting the occurrence
probability of a certain user action for the tweet. There will be a separate predictor
for like, comment, time spent, share, hide, and report. The results of these models can
be merged, each having a different weight/importance, to generate a rank score. The
Tweets will then be ranked according to this score.

Separately predicting each user action allows us to have greater control over the
importance we want to give to each action when calculating the rank of the Tweet. We can
tweak the weights to display Tweets in such a manner that would align with our current
business objectives, i.e., give certain user actions higher/lower weight according to
the business needs.

Training data generation
------------------------

Each user engagement action on the Twitter feed will generate positive and negative
training examples for the user engagement prediction models.

Tweet Selection
===============

Tweet selection is dynamic in nature. The Tweets selected will continue to change each
time.

New Tweets
----------

Consider the following scenario to see how Tweet selection occurs.

User A logs in to Twitter at 9 am to view their Twitter feed. Now, the Tweet selection
component has to fetch Tweets for display. It fetches the five-hundred newly generated
Tweets by A’s network since the last login at 3 pm yesterday.

New Tweets + unseen Tweets
--------------------------

The new Tweets fetched in the previous step are ranked and displayed on user A’s feed.
They only view the first two-hundred Tweets and log out. The ranked results are also
stored in a cache.

Now, user A logs in again at 10 pm. According to the last scheme, the Tweet selection
component should select all the new Tweets generated between 9 am and 10 pm. However,
this may not be the best idea!

For 9am’s feed, the component previously selected a Tweet made at 8:45 am. Since this
Tweet was recently posted, it did not have much user engagement at that time. Therefore,
it was ranked at the 450th position in the feed. Now, remember that A logged out after
only viewing the first two-hundred Tweets and this Tweet remained unread. Since the
user’s last visit, this unread Tweet gathered a lot of attention in the form of
reshares, likes, and comments. The Tweet selection component should now reconsider this
Tweet’s selection. This time it would be ranked higher, and A will probably view it.

Keeping the above rationale in mind, the Tweet selection component now fetches a mix of
newly generated Tweets along with a portion of unseen Tweets from the cache.

Edge case: User returning after a while
---------------------------------------

Consider a scenario, where user A might log in after two weeks. A lot of Tweets would
have been generated since A’s last login. Therefore, the Tweet selection component will
impose a limit on the Tweet data it will select. The main focus is that the pool of
Tweets keeps on increasing so a limit needs to be imposed on the number of Tweets that
the component will fetch.

Network Tweets + interest / popularity-based Tweets
---------------------------------------------------

There could be Tweets outside of user A’s network that have a high potential of engaging
them. Hence, we arrive at a two-dimensional scheme of selecting network Tweets and
potentially engaging Tweets.

An engaging Tweet could be one that:

    - aligns with user A’s interests

    - is locally/globally trending

    - engages user A’s network

Selecting these Tweets can prove to be very beneficial in two cases:

    1. The user has recently joined the platform and follows only a few others. As such,
their small network is not able to generate a sufficient number of Tweets for the Tweet
selection component (known as the Bootstrap problem).

    2. The user likes a Tweet from a person outside of his network and decides to add
them to their network. This would increase the discoverability on the platform and help
grow the user’s network.

Feature Engineering
===================

The machine learning model is required to predict user engagement on user A’s Twitter
feed.

Let’s begin by identifying the four main actors in a twitter feed:

    1. The logged-in user

    2. The Tweet

    3. Tweet’s author

    4. The context

Dense features
--------------

User-author features
--------------------

These features are based on the logged-in user and the Tweet’s author. They will capture
the social relationship between the user and the author of the Tweet, which is an
extremely important factor in ranking the author’s Tweets. For example, if a Tweet is
authored by a close friend, family member, or someone that user is highly influenced by,
there is a high chance that the user would want to interact with the Tweet.

User-author historical interactions
    - author_liked_posts_3months

    - author_liked_posts_count_1year

User-author similarity

Another immensely important feature set to predict user engagement focuses on figuring
out how similar the logged-in user and the Tweet’s author are.
    - common_followees

    - topic_similarity

        - followed by the logged-in user and author

        - present in the posts that the logged-in user and author have interacted with
in the past

        - used by the author and logged-in user in their posts

    - tweet_content_embedding_similarity. A user is represented by the content that they
have generated and interacted with in the past. You can utilize all of that content as a
bag-of-words and build an embedding for every user. With an embedding vector for each
user, the dot product between them can be used as a fairly good estimate of
user-to-author similarity.

    - social_embedding_similarity. Another way to capture the similarity between the
user and the author is to generate embeddings based on the social graph rather than
based on the content of Tweets. The basic notion is that people who follow the same or
similar topics or influencers are more likely to engage with each other’s content. A
basic way to train this model is to represent each user with all the other users and
topics (user and topic ids) that they follow in the social graph. Essentially, every
user is represented by bag-of-ids (rather than bag-of-words), and you use that to train
an embedding model. The user-author similarity will then be computed between their
social embeddings and used as a signal.

User-Tweet features
-------------------

The similarity between the user’s interests and the tweet’s topic is also a good
indicator of the relevance of a Tweet.

    - topic_similarity. You can use the hashtags and/or the content of the Tweets that
the user has either Tweeted or interacted with, in the last six months and compute the
TF-IDF similarity with the Tweet itself. This indicates whether the Tweet is based on a
topic that the user is interested in.

    - embedding_similarity. Another option to find the similarity between the user’s
interest and the Tweet’s content is to generate embeddings for the user and the Tweet.
The Tweet’s embedding can be made based on the content and hashtags in it. While the
user’s embedding can be made based on the content and hashtags in the Tweets that they
have written or interacted with.

Author features
---------------

Author’s degree of influence
    - is_verified

    - author_social_rank

    - author_num_followers

    - follower_to_following_ratio

        1. The type of user account

        2.The account’s influence

        3. The quality of the account’s content (Tweets)

Historical trend of interactions on the author’s Tweets
-------------------------------------------------------

Another very important set of features is the interaction history on the author’s
Tweets. If historically, an author’s Tweets garnered a lot of attention, then it is
highly probable that this will happen in the future, too. A high rate of historical
interaction for a user implies that the user posts high-quality content.


    - author_engagement_rate_3months

    - author_topic_engagement_rate_3months

Tweet features
--------------

Features based on Tweet’s content

    - Tweet_length

    - Tweet_recency

    - is_image_video

    - is_URL

        1. Calls for action

        2. Provides valuable information

Features based on Tweet’s interaction

Tweets with a greater volume of interaction have a higher probability of engaging the
user.

    - num_total_interactions

    We can apply a simple time decay model to weight the latest interaction more than
the ones that happened some time ago. Time decay can be used in all features where there
is a decline in the value of a quantity over time. One simple model can be to weight
every interaction (like, comment, and retweet) by 1 / (t + 1), where t is the number of
days from current time.

    Another remedy is to use different time windows to capture the recency of
interactions while looking at their numbers. The interaction in each window can be used
as a feature:

    - interactions_in_last_1_hour

    - interactions_in_last_1_day

Separate features for different engagements

    - likes_in_last_3_days

    - comments_in_last_1_day

    - reshares_in_last_2_hours

    - likes_in_last_3_days_user’s_network_only

    - comments_in_last_1_day_user’s_network_only

    - reshares_in_last_2_hours_user’s_network_only

Context-based features
----------------------

    - day_of_week

    - time_of_day

    - current_user_location

    - season

    - lastest_k_tag_interactions

    - approaching_holiday

Sparse features
---------------

    - unigrams/bigrams of a Tweet. The presence of certain unigrams or bigrams may
increase the probability of engagement for a tweet. For example, during data
visualisation, you may observe that Tweets with the bigram “Order now” have higher user
engagement. The reason behind this might be that such Tweets are useful and informative
for users as they redirect them towards websites where they can purchase things
according to their needs.

    - user_id

    - tweets_id

Training Data Generation
========================

Training data generation through online user engagement
-------------------------------------------------------

If you are training a single model to predict user engagement, then all the Tweets that
received user engagement would be labeled as positive training examples. Similarly, the
Tweets that only have impressions would be labeled as negative training examples.

Impression: If a Tweet is displayed on a user’s Twitter feed, it counts as an
impression. It is not necessary that the user reads it or engages with it, scrolling
past it also counts as an impression.

You can train different models, each to predict the probability of occurrence of
different user actions on a tweet. When you generate data for the “Like” prediction
model, all Tweets that the user has liked would be positive examples, and all the Tweets
that they did not like would be negative examples.

Balancing positive and negative training examples
-------------------------------------------------

In the feed-based system scenario, on average, a user engages with as little as
approximately 5% of the Tweets that they view per day. Therefore, in order to balance
the ratio of positive and negative training samples, you can randomly downsample:

    - negative examples to five million samples

    - positive examples to five million samples

Now, you would have a total of ten million training examples per day; five million of
which are positive and five million are negative.

If a model is well-calibrated, the distribution of its predicted probability is similar
to the distribution of probability observed in the training data. However, as we have
changed the sampling of training data, our model output scores will not be
well-calibrated. For example, for a tweet that got only 5% engagement, the model might
predict 50% engagement. This would happen because the model is trained on data that has
an equal quantity of negative and positive samples. So the model would think that it is
as likely for a tweet to get engagement as it is to be ignored. However, we know from
the training that this is not the case. Given that the model’s scores are only going to
be used to rank Tweets among themselves, poor model calibration doesn’t matter much in
this scenario.

Train test split
----------------

You need to be mindful of the fact that the user engagement patterns may differ
throughout the week. Hence, you will use a week’s engagement to capture all the patterns
during training data generation.

Random splitting defeats the purpose of training the model on an entire week’s data.
Also, the data has a time dimension, i.e., we know the engagement on previous Tweets,
and we want to predict the engagement on future Tweets ahead of time. Therefore, you
will train the model on data from one time interval and validate it on the data with the
succeeding time interval.

Ranking
=======

The task at hand is to predict the probability of different engagement actions for a
given Tweet. So, essentially, your models are going to predict the probability of likes,
comments and retweets, i.e., P(click), P(comments) and P(retweet). Looking at the
training data, we know that this is a classification problem where we want to minimize
the classification error or maximize area under the curve (AUC).

Logistic regression
-------------------

Initially, a simple model that makes sense to train is a logistic regression model with
regularization.

A key advantage of using logistic regression is that it is reasonably fast to train.
This enables you to test new features fairly quickly to see if they make an impact on
the AUC or validation error. Also, it’s extremely easy to understand the model. You can
see from the feature weights which features have turned out to be more important than
others.

A major limitation of the linear model is that it assumes linearity exists between the
input features and prediction. Therefore, you have to manually model feature
interactions. For example, if you believe that the day of the week before a major
holiday will have a major impact on your engagement prediction, you will have to create
this feature in your training data manually. Other models like tree-based and neural
networks are able to learn these feature interactions and utilize them effectively for
predictions.

Another key question is whether you want to train a single classifier for overall
engagement or separate ones for each engagement action based on production needs. In a
single classifier case, you can train a logistic regression model for predicting the
overall engagement on a Tweet. Tweets with any form of engagement will be considered as
positive training examples for this approach.

Another approach is to train separate logistic regression models to predict P(like),
P(comments) and P(retweet). These models will utilize the same features. However, the
assignments of labels to the training examples will differ. Tweets with likes, comments,
or retweets will be considered positive examples for the overall engagement predictor
model. Only Tweets with likes will be considered as positive examples for the “Like”
predictor model, etc.

MART
----

Another modeling option that should be able to outperform logistic regression with dense
features is additive tree-based models, e.g., Boosted Decision Trees and Random Forest.
Trees are inherently able to utilize non-linear relations between features that aren’t
readily available to logistic regression.

Tree-based models also don’t require a large amount of data as they are able to
generalize well quickly. So, a few million examples should be good enough to give us an
optimized model.

There are various hyperparameters that you might want to play around to get to an
optimized model, including

    - Number of trees

    - Maximum depth

    - Minimum samples needed for split

    - Maximum features sampled for a split

Approach 1

A simple approach is to train a single model to predict the overall engagement.

Approach 2

You could have specialized predictors to predict different kinds of engagement.

Approach 3

Consider a scenario, where a person reshares a Tweet but does not click the like button.
Even though the user didn’t actually click on the like button, retweeting generally
implies that the user likes the Tweet. The positive training example for the retweet
model may prove useful for the like model as well. Hence, you can reuse all positive
training examples across every model.

One way to utilize the overall engagement data among each individual predictor of
P(like), P(comment) and P(retweet) is to build one common predictor, i.e., P(engagement)
and share its output as input into all of your predictors.

To take approach three a notch further, you can use the output of each tree in the
“overall engagement predictor” ensemble as features in the individual predictors. This
allows for even better learning as the individual model will be able to learn from the
output of each individual tree.

Deep learning
-------------

Training the model as well as evaluating the model at feed generation time can make this
approach computationally very expensive. So, you may want to fall back to the
multi-layer approach, i.e., having a simpler model for stage one ranking and use complex
stage two model to obtain the most relevant Tweets ranked at the top of the user’s
Twitter feed.

Like with the tree-based approach, there are quite a few hyperparameters that you should
tune for deep learning to find the most optimized model that minimizes the test set
error. They are:

    - Learning rate

    - Number of hidden layers

    - Batch size

    - Number of epochs

    - Dropout rate for regularizing model and avoiding overfitting

Separate neutral networks
-------------------------

One way is to train separate neural nets for each of the P(like), P(comment) and
P(retweet). However, for a very large scale data set, training separate deep neural
networks (NNs) can be slow and take a very long time (ten’s of hours to days).

Multi-task neural networks
--------------------------

Likes, comments, and retweets are all different forms of engagement on a Tweet.
Therefore, predicting P(like), P(comment) and P (retweet) are similar tasks. When
predicting similar tasks, you can use multitask learning. Hence, you can train a neural
network with shared layers (for shared knowledge) appended with specialized layers for
each task’s prediction. The weights of the shared layers are common to the three tasks.
Whereas in the task-specific layer, the network learns information specific to the
tasks. The loss function for this model will be the sum of individual losses for all the
tasks:

total_loss = like_loss + comment_loss + retweet_loss

The model will then optimize for this joint loss leading to regularization and joint
learning.

This approach should be able to perform at least as effective as training separate
networks for each task. It should be able to outperform in most cases as we use the
shared data and use it across each predictor. Also, one key advantage of using shared
layers is that models would be much faster to train than training completely separate
deep neural networks for each task.

Stacking models and online learning
-----------------------------------

One way to outperform the “single model technique approach” is to use multiple models to
utilize the power of different techniques by stacking models on top of each other.

The setup includes training tree-based models and neural networks to generate features
that you will utilize in a linear model (logistic regression). The main advantage of
doing this is that you can utilize online learning, meaning we can continue to update
the model with every user action on the live site. You can also use sparse features in
our linear model while getting the power of all non-linear relations and functions
through features generated by tree-based models and neural networks.

For trees, you can generate features by using the triggering of leaf nodes, e.g., given
two trees with four leaf nodes, each will result in eight features that can be used in
logistic regression. Each leaf node in the random forest will result in a boolean
feature that will be active based on whether the leaf node is triggered or not.

Similarly, for neutral networks, rather than predicting the probability of events, you
can just plug-in the output of the last hidden layer as features into the logistic
regression models.

To summarize, this stacking model setup will still give us all the learning power of
deep neural networks and tree-based models along with the flexibility of training
logistic regressions model, while keeping it almost real-time refreshed with online
learning.

Another advantage of using real-time online learning with logistic regression is that
you can also utilize sparse features to learn the interaction, e.g., features like
user_id and tweet_id can be used to memorize the interaction with each individual user
and Tweet.

Given that features like tweet_id and user_id are extremely sparse, training and
evaluation of the model must be done in a distributed environment because the data won’t
fit on one machine.

Diversity
=========

Why do you need diverse Tweets?
-------------------------------

Consider a scenario where the sorted list of Tweets has five consecutive posts by the
same author. No, your ranking model hasn’t gone bonkers! It has rightfully placed these
tweets at the top because:

    - The logged-in user and the Tweet’s author have frequently interacted with each
other’s Tweets

    - The logged-in user and the Tweet’s author have a lot in common like hashtags
followed and common followees

    - The author is very influential, and their Tweets generally gain a lot of traction

Diversity in Tweets’ authors
----------------------------

However, no matter how good of a friend the author is or how interesting their Tweets
might be, user A would eventually get bored of seeing Tweets from the same author
repeatedly. Hence, you need to introduce diversity with regards to the Tweets’ author.

Diversity in tweets’ content
----------------------------

Another scenario where we might need to introduce diversity is the Tweet’s content. For
instance, if your sorted list of Tweets has four consecutive tweets that have videos in
them, the user might feel that their feed has too many videos.

Introducing the repetition penalty
----------------------------------

One way to introduce a repetition penalty could be to add a negative weight to the
Tweet’s score upon repetition. For instance, whenever you see the author being repeated,
you add a negative weight of -0.1 to the Tweet’s score.

Another way to achieve the same effect is to bring the Tweet with repetition three steps
down in the sorted list. For instance, when you observe that two consecutive Tweets have
media content in them, you bring the latter down by three steps.

Online Experimentation
======================

Step 1: Training different models
---------------------------------

After the split, the training data is utilized to train, say, fifteen different models,
each with a different combination of hyperparameters, features, and machine learning
algorithms.

Step 2: Validating models offline
---------------------------------

Once these fifteen models have been trained, you will use the validation data to select
the best model offline.

Step 3: Online experimentation
------------------------------

Now that you have selected the best model offline, you will use A/B testing to compare
the performance of this model with the currently deployed model, which displays the feed
in reverse chronological order. You will select 1% of the five-hundred million active
users, i.e., five million users for the A/B test. Two buckets of these users will be
created each having 2.5 million users. Bucket one users will be shown twitter timelines
according to the time-based model; this will be the control group. Bucket two users will
be shown the Twitter timeline according to the new ranking model.

However, before you perform this A/B test, you need to retrain the ranking model.

Recall that you withheld the most recent partition of the training data to use for
validation and testing. This was done to check if the model would be able to predict
future engagements on tweets given the historical data. However, now that you have
performed the validation and testing, you need to retrain the model using the recent
partitions of training data so that it captures the most recent phenomena.

Step 4: To deploy or not to deploy
----------------------------------

The results of the A/B tests will help decide whether you should deploy the new ranking
model across the platform.

You can observe that the Twitter feeds generated by the new ranking model had thirty
(180k - 150k) more engagements.

Increase in engagement (gain) = (180k - 150k) / 150k = 20%

This model is clearly able to outperform the current production, or live state. You
should use statistical significance (like p-value) to ensure that the gain is real.

Another aspect to consider when deciding to launch the model on production, especially
for smaller gains, is the increase in complexity. If the new model increases the
complexity of the system significantly without any significant gains, you should not
deploy it.

You should go for complex solutions (based on new features, or data, etc.) only if you
anticipate it to bring larger gains in the future.

To wrap up, if, after an A/B experiment, you see an engagement gain by the model that is
statistically significant and worth the complexity it adds to the system, it makes sense
to replace the current live system with the new model.