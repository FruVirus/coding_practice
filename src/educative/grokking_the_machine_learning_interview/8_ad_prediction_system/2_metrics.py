"""
Metrics
=======

The metrics should ensure that these models help the overall improvement of the
platform, increase revenue, and provide value for the advertisers.

In our case, it’s a binary classification task; a user engages with ad or not. We use a
0 class label for no-engagement (with an ad) and 1 class label for engagement (with an
ad).

Like any other optimization problem, there are two types of metrics to measure the
effectiveness of our ad prediction system:

    1. Offline metrics

    2. Online metrics

Calibration
-----------

Calibration measures the ratio of average predicted rate and average empirical rate. In
other words, it is the ratio of the number of expected actions to the number of actually
observed actions.

Calibration = predicted rate / actual historically observed rate

Why do we need calibration? When we have a significant class imbalance, i.e., the
distribution is skewed towards positive and negative class, we calibrate our model to
estimate the likelihood of a data point belonging to a class.

Offline metrics
---------------

Log Loss

Let’s first go over the area under the receiver operator curve (AUC), which is a
commonly used metric for model comparison in binary classification tasks. However, given
that the system needs well-calibrated prediction scores, AUC, has the following
shortcomings in this ad prediction scenario.

    1. AUC does not penalize for “how far off” predicted score is from the actual label.
For example, let’s take two positive examples (i.e., with actual label 1) that have the
predicted scores of 0.51 and 0.7 at threshold 0.5. These scores will contribute equally
to our loss even though one is much closer to our predicted value.

    2. AUC is insensitive to well-calibrated probabilities.

Since, in our case, we need the model’s predicted score to be well-calibrated to use in
Auction, we need a calibration-sensitive metric. Log loss should be able to capture this
effectively as Log loss (or more precisely cross-entropy loss) is the measure of our
predictive error.

This metric captures to what degree expected probabilities diverge from class labels. As
such, it is an absolute measure of quality, which accounts for generating
well-calibrated, probabilistic output.

Online metrics
--------------

Overall revenue

This captures the overall revenue generated by the system for the cohorts of the user in
either an experiment or, more generally, to measure the overall performance of the
system. It’s important to call out that just measuring revenue is a very short term
approach, as we may not provide enough value to advertisers and they will move away from
the system. However, revenue is definitely one critical metric to track.

Revenue is basically computed as the sum of the winning bid value (as selected by
auction) when the predicted event happens, e.g., if the bid amounts to $0.5 and the user
clicks on the ad, the advertiser will be charged $0.5. The business won’t be charged if
the user doesn’t click on the ad.

Overall ads engagement rate

Engagement rate measures the overall action rate, which is selected by the advertiser.
Some of the actions might be:

    1. Click rate. This will measure the ratio of user clicks to ads.

    2. Downstream action rate. This will measure the action rate of a particular action
targeted by the advertiser e.g. add to cart rate, purchase rate, message rate etc.

Counter metrics

It’s important to track counter metrics to see if the ads are negatively impacting the
platform.

We want the users to keep showing engagement with the platform and ads should not hinder
that interest. That is why it’s important to measure the impact of ads on the overall
platform as well as direct negative feedback provided by users. There is a risk that
users can leave the platform if ads degrade the experience significantly.

So, for online ads experiments, we should track key platform metrics, e.g., for search
engines, is session success going down significantly because of ads? Are the average
queries per user impacted? Are the number of returning users on the platform impacted?
These are a few important metrics to track to see if there is a significant negative
impact on the platform.

Along with top metrics, it’s important to track direct negative feedback by the user on
the ad such as providing following feedback on the ad:

    1. Hide ad

    2. Never see this ad

    3. Report ad as inappropriate

These negative sentiments can lead to the perceived notion of the product as negative.
"""
